package main

import (
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"sync"
	"sync/atomic"
	"time"
)

// runPortChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡Œç«¯å£ç›‘æ§
func runPortChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.PortMonitor {
		return
	}
	if !portRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorPorts ] already running, skip this round")
		return
	}
	startTime := time.Now()

	log.Println("[ monitorPorts ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorPorts ] function completed (duration: %.4fs)", duration.Seconds())
		portRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				// åªæ‰§è¡Œç«¯å£æ£€æŸ¥ç›¸å…³é€»è¾‘
				pingState := checkPingState(job.address)
				if !pingState {
					continue // æœåŠ¡å™¨ä¸é€šï¼Œè·³è¿‡
				}
				portState := checkPort(job.address, job.port)

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handlePortStatus(job.address, job.port, job.key, portState, status, config)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributePortJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorPorts ] function completed")
}

// runProcessChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡Œè¿›ç¨‹ç›‘æ§
func runProcessChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.ProcessMonitor {
		return
	}
	if !processRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorProcesses ] already running, skip this round")
		return
	}
	startTime := time.Now()
	log.Println("[ monitorProcesses ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorProcesses ] function completed (duration: %.4fs)", duration.Seconds())
		processRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				// åªæ‰§è¡Œè¿›ç¨‹æ£€æŸ¥ç›¸å…³é€»è¾‘
				pingState := checkPingState(job.address)
				if !pingState {
					continue // æœåŠ¡å™¨ä¸é€šï¼Œè·³è¿‡
				}
				statusResponse, err := getCachedCheckData(job.address)
				if err != nil {
					//log.Printf("Error getting check data for %s: %v", job.address, err)
					continue
				}

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handleProcessStatus(job.address, job.key, statusResponse.ProcessStatuses, status, config)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributeAddressJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorProcesses ] function completed")
}

// runPingChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡ŒæœåŠ¡å™¨é€šä¿¡çŠ¶æ€ç›‘æ§
func runPingChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.ServerReachMonitor {
		return
	}
	if !pingRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorPorts ] already running, skip this round")
		return
	}
	startTime := time.Now()

	log.Println("[ monitorPing ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorPing ] function completed (duration: %.4fs)", duration.Seconds())
		pingRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				pingState := checkPingState(job.address)

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handlePingStatus(job.address, job.key, pingState, status, config)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributeAddressJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorPing ] function completed")
}

// runResourceChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡ŒæœåŠ¡å™¨èµ„æºçŠ¶æ€ç›‘æ§
func runResourceChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.Resource {
		return
	}
	if !resourceRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorPorts ] already running, skip this round")
		return
	}
	startTime := time.Now()

	log.Println("[ monitorResource ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorResource ] function completed (duration: %.4fs)", duration.Seconds())
		resourceRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				pingState := checkPingState(job.address)
				if !pingState {
					continue // æœåŠ¡å™¨ä¸é€šï¼Œè·³è¿‡
				}
				statusResponse, err := getCachedCheckData(job.address)
				if err != nil {
					//log.Printf("Error getting check data for %s: %v", job.address, err)
					continue
				}

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handleResourceStatus(job.address, job.key, statusResponse.Metrics, job.server, status, config)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributeAddressJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorResource ] function completed")
}

// runDirectoryChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡ŒæœåŠ¡å™¨èµ„æºçŠ¶æ€ç›‘æ§
func runDirectoryChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.DirFileMonitor {
		return
	}
	if !directoryRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorPorts ] already running, skip this round")
		return
	}
	startTime := time.Now()

	log.Println("[ monitorDirectory ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorDirectory ] function completed (duration: %.4fs)", duration.Seconds())
		directoryRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)
	now := time.Now()

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				pingState := checkPingState(job.address)
				if !pingState {
					continue // æœåŠ¡å™¨ä¸é€šï¼Œè·³è¿‡
				}
				statusResponse, err := getCachedCheckData(job.address)
				if err != nil {
					//log.Printf("Error getting check data for %s: %v", job.address, err)
					continue
				}

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handleDirectoryStatus(job.address, job.key, statusResponse.DirectoryStatuses, status, config, now)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributeAddressJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorDirectory ] function completed")
}

// runTargetPortChecks è´Ÿè´£ç‹¬ç«‹æ‰§è¡ŒæœåŠ¡å™¨èµ„æºçŠ¶æ€ç›‘æ§
func runTargetPortChecks(config Config, statuses map[string]*ServerStatus) {
	if !config.Monitor.RaPortMonitor {
		return
	}
	if !targetPortRunning.CompareAndSwap(false, true) {
		//log.Println("[ monitorPorts ] already running, skip this round")
		return
	}
	startTime := time.Now()

	log.Println("[ monitorTargetPort ] function starts")
	defer func() {
		duration := time.Since(startTime)
		log.Printf("[ monitorTargetPort ] function completed (duration: %.4fs)", duration.Seconds())
		targetPortRunning.Store(false)
	}()

	var wg sync.WaitGroup
	jobChan := make(chan serverJob, maxWorkers)

	// å¯åŠ¨ worker
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for job := range jobChan {
				pingState := checkPingState(job.address)
				if !pingState {
					continue // æœåŠ¡å™¨ä¸é€šï¼Œè·³è¿‡
				}
				statusResponse, err := getCachedCheckData(job.address)
				if err != nil {
					//log.Printf("Error getting check data for %s: %v", job.address, err)
					continue
				}

				// è·å–çŠ¶æ€å¯¹è±¡
				statusesMutex.Lock()
				status, ok := statuses[job.key]
				if !ok { // å¦‚æœ map ä¸­ä¸å­˜åœ¨è¿™ä¸ª key
					// åˆ›å»ºæ–°çš„ status å¯¹è±¡
					status = &ServerStatus{
						PortAlertSent:       0,
						ProcessAlertSent:    make(map[string]int32),
						CpuAlertSent:        0,
						MemAlertSent:        0,
						DiskAlertSent:       make(map[string]int32),
						DirAlertSent:        make(map[string]int32),
						FileAlertSent:       make(map[string]int32),
						PingAlertSent:       0,
						TargetPortAlertSent: make(map[string]int32),
					}
					// å°†æ–°åˆ›å»ºçš„å¯¹è±¡å­˜å…¥ map
					statuses[job.key] = status
				}
				statusesMutex.Unlock()

				handleTargetPortStatus(job.address, job.key, statusResponse.PortStatuses, status, config)
			}
		}()
	}

	// åˆ†å‘ä»»åŠ¡
	distributeAddressJobs(config, jobChan)
	wg.Wait()
	//log.Println("[ monitorTargetPort ] function completed")
}

// å¯é€‰ï¼šåˆ›å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥åˆ†å‘ä»»åŠ¡ï¼Œé¿å…ä»£ç é‡å¤
func distributePortJobs(config Config, jobChan chan<- serverJob) {
	// éå†æ‰€æœ‰æœåŠ¡å™¨ã€åœ°å€å’Œç«¯å£æ¥åˆ›å»ºä»»åŠ¡
	for _, server := range config.Servers {
		for _, address := range server.Addresses {
			// å¯¹äºè¿›ç¨‹ã€èµ„æºç­‰éç«¯å£ç‰¹å®šçš„ç›‘æ§ï¼Œæˆ‘ä»¬åªéœ€è¦æ¯ä¸ªåœ°å€ä¸€ä¸ªä»»åŠ¡
			// è¿™é‡Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä»ç„¶ç”¨ address:port ä½œä¸º keyï¼Œä½†å¯ä»¥ä¼˜åŒ–
			if len(server.Ports) == 0 { // å¦‚æœæ²¡æœ‰ç«¯å£ï¼Œè‡³å°‘ä¸ºåœ°å€åˆ›å»ºä¸€ä¸ªä»»åŠ¡
				key := address
				jobChan <- serverJob{address: address, server: server, key: key}
			} else {
				for _, port := range server.Ports {
					if strings.TrimSpace(port) == "" {
						continue
					}
					key := fmt.Sprintf("%s:%s", address, port)
					jobChan <- serverJob{address: address, port: port, server: server, key: key}
				}
			}
		}
	}
	close(jobChan)
}

// distributeAddressJobs ä¸ºæ¯ä¸ªæœåŠ¡å™¨åœ°å€åˆ›å»ºä¸€ä¸ªä»»åŠ¡
func distributeAddressJobs(config Config, jobChan chan<- serverJob) {
	// åˆ›å»ºä¸€ä¸ª map æ¥é˜²æ­¢é‡å¤æ·»åŠ ç›¸åŒçš„åœ°å€
	uniqueAddresses := make(map[string]Server)

	for _, server := range config.Servers {
		for _, address := range server.Addresses {
			uniqueAddresses[address] = server
		}
	}

	// ä¸ºæ¯ä¸ªå”¯ä¸€çš„åœ°å€åˆ†å‘ä¸€ä¸ª job
	for addr, srv := range uniqueAddresses {
		jobChan <- serverJob{address: addr, server: srv, key: addr} // key ç›´æ¥ä½¿ç”¨ address
	}
	close(jobChan)
}

// è·å–å¸¦ç¼“å­˜çš„æ£€æŸ¥æ•°æ®
func getCachedCheckData(address string) (*StatusResponse, error) {
	// 1) ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
	if cached, ok := cachedChecks.Load(address); ok {
		if lastTime, ok := lastCheckTime.Load(address); ok {
			if time.Since(lastTime.(time.Time)) < time.Second*time.Duration(checkCacheTTL) {
				return cached.(*StatusResponse), nil
			}
		}
	}

	// 2) æ£€æŸ¥å†·å´çŠ¶æ€
	if val, ok := lastFailure.Load(address); ok {
		if record, ok2 := val.(FailureRecord); ok2 {
			cooldown := time.Duration(config.FailureCooldown) * time.Second
			if time.Since(record.LastFail) < cooldown {
				// å†·å´æœŸå†…ï¼Œä»…é¦–æ¬¡æç¤ºä¸€æ¬¡
				if !record.Notified {
					log.Printf("Error getting check data for %s: recent failure, short-circuited", address)
					record.Notified = true
					lastFailure.Store(address, record)
				}
				return nil, fmt.Errorf("recent failure, short-circuited: %s", address)
			} else {
				// å†·å´è¿‡æœŸ â†’ åˆ é™¤è®°å½•
				lastFailure.Delete(address)
			}
		}
	}

	// 3) å¹¶å‘é™åˆ¶
	select {
	case outboundSem <- struct{}{}:
		defer func() { <-outboundSem }()
	case <-time.After(2 * time.Second):
		return nil, fmt.Errorf("outbound concurrency busy")
	}

	// 4) å¸¦è¶…æ—¶çš„è¯·æ±‚
	ctx, cancel := context.WithTimeout(context.Background(), time.Duration(config.HttpTimeout)*time.Second)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, "GET", fmt.Sprintf("http://%s:9600/check", address), nil)
	if err != nil {
		lastFailure.Store(address, FailureRecord{LastFail: time.Now(), Notified: false})
		return nil, err
	}

	resp, err := httpClient.Do(req)
	if err != nil {
		lastFailure.Store(address, FailureRecord{LastFail: time.Now(), Notified: false})
		return nil, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		lastFailure.Store(address, FailureRecord{LastFail: time.Now(), Notified: false})
		return nil, fmt.Errorf("bad status: %d, body: %s", resp.StatusCode, string(body))
	}

	// 5) è§£æå“åº”
	bodyData, err := io.ReadAll(resp.Body)
	if err != nil {
		lastFailure.Store(address, FailureRecord{LastFail: time.Now(), Notified: false})
		return nil, err
	}

	var statusResponse StatusResponse
	if err := json.Unmarshal(bodyData, &statusResponse); err != nil {
		lastFailure.Store(address, FailureRecord{LastFail: time.Now(), Notified: false})
		return nil, err
	}

	// 6) æˆåŠŸ â†’ æ¸…ç†å¤±è´¥è®°å½• & æ›´æ–°ç¼“å­˜
	cachedChecks.Store(address, &statusResponse)
	lastCheckTime.Store(address, time.Now())
	lastFailure.Delete(address)

	return &statusResponse, nil
}

// å¤„ç†PingçŠ¶æ€
func handlePingStatus(address, key string, pingState bool, status *ServerStatus, config Config) {
	if !config.Monitor.ServerReachMonitor {
		return
	}

	pingAlertSent := atomic.LoadInt32(&status.PingAlertSent) == 1

	if !pingState && !pingAlertSent {
		data := EmailTemplateData{
			Subject:   "ğŸš¨æœåŠ¡å™¨å¤±è”å‘Šè­¦",
			Server:    address,
			Message:   "æœåŠ¡å™¨é€šä¿¡å¤±è”ï¼Œå¦‚å½±å“ä¸šåŠ¡è¯·åŠæ—¶å¤„ç†ï¼",
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "critical", data)
		log.Printf("æœåŠ¡å™¨ %s é€šè¿‡22ç«¯å£æ£€æµ‹é€šä¿¡å¤±è”ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€", address)
		atomic.StoreInt32(&status.PingAlertSent, 1)
		//fmt.Println(data.Message)
	} else if pingState && pingAlertSent {
		data := EmailTemplateData{
			Subject:   "âœ…æœåŠ¡å™¨é€šä¿¡æ¢å¤",
			Server:    address,
			Message:   "æœåŠ¡å™¨é€šä¿¡å·²æ¢å¤æ­£å¸¸ï¼Œè¯·æ£€æŸ¥æ‰€æ‰¿è½½ä¸šåŠ¡æ˜¯å¦å·²æ­£å¸¸å¯åŠ¨ï¼",
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "recovery", data)
		log.Printf("æœåŠ¡å™¨ %s é€šè¿‡22ç«¯å£æ£€æµ‹é€šä¿¡å·²æ¢å¤ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€", address)
		atomic.StoreInt32(&status.PingAlertSent, 0)
		//fmt.Println(data.Message)
	}

}

// å¤„ç†ç«¯å£çŠ¶æ€
func handlePortStatus(address, port, key string, portState bool, status *ServerStatus, config Config) {
	portAlertSent := atomic.LoadInt32(&status.PortAlertSent) == 1

	if !portState && !portAlertSent {
		data := EmailTemplateData{
			Subject:   "âš ï¸âš ï¸ç«¯å£å¤±è”å‘Šè­¦",
			Server:    address,
			Message:   fmt.Sprintf("æœåŠ¡å™¨ç«¯å£ %s é€šä¿¡å¤±è”", port),
			Action:    "è¯·æ£€æŸ¥ç«¯å£ç›¸å…³çš„è¿›ç¨‹æ˜¯å¦æœ‰å­˜åœ¨ï¼Œæˆ–è€…æ˜¯å¦æœ‰é‡å¯ç°è±¡ï¼",
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "severe", data)
		atomic.StoreInt32(&status.PortAlertSent, 1)
		log.Printf("æœåŠ¡å™¨ %s ç«¯å£ %s é€šè¿‡ tcp+udp æ£€æµ‹ç«¯å£å¤±è”ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€", address, port)
		//fmt.Println(data.Message)
	} else if portState && portAlertSent {
		data := EmailTemplateData{
			Subject:   "âœ…ç«¯å£é€šä¿¡æ¢å¤",
			Server:    address,
			Message:   fmt.Sprintf("æœåŠ¡å™¨ç«¯å£ %s é€šä¿¡å·²æ¢å¤æ­£å¸¸ã€‚", port),
			Action:    "æœåŠ¡å™¨ç«¯å£å·²æ¢å¤æ­£å¸¸ï¼Œè¯·çŸ¥æ‚‰ï¼",
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "recovery", data)
		log.Printf("æœåŠ¡å™¨ %s ç«¯å£ %s é€šè¿‡ tcp+udp æ£€æµ‹ç«¯å£é€šä¿¡æ¢å¤ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€", address, port)
		atomic.StoreInt32(&status.PortAlertSent, 0)
		//fmt.Println(data.Message)
	}
}

// å¤„ç†è¿›ç¨‹çŠ¶æ€
func handleProcessStatus(address, key string, processes []ProcessStatus, status *ServerStatus, config Config) {
	for _, result := range processes {
		processName := result.ProcessName
		isRunning := result.IsRunning

		// ä½¿ç”¨åŸå­æ“ä½œç®¡ç†çŠ¶æ€
		statusMap := &status.ProcessAlertSent
		statusMutex := &status.ProcessMutex

		statusMutex.Lock()
		alertSent, exists := (*statusMap)[processName]
		if !exists {
			(*statusMap)[processName] = 0
			alertSent = 0
		}

		if !isRunning && alertSent == 0 {
			data := EmailTemplateData{
				Subject:   "âš ï¸âš ï¸è¿›ç¨‹æ¶ˆå¤±å‘Šè­¦",
				Server:    address,
				Message:   fmt.Sprintf("æœåŠ¡å™¨è¿›ç¨‹ %s ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ï¼", processName),
				Action:    "è¯·ç™»å½•æœåŠ¡å™¨æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜åœ¨ï¼Œæˆ–è€…æ˜¯å¦æœ‰é‡å¯ç°è±¡ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "severe", data)
			(*statusMap)[processName] = 1
			log.Printf("æœåŠ¡å™¨ %s è¿›ç¨‹ %s ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€", address, processName)
			//fmt.Println(data.Message)
		} else if isRunning && alertSent == 1 {
			data := EmailTemplateData{
				Subject:   "âœ…è¿›ç¨‹å·²å¯åŠ¨",
				Server:    address,
				Message:   fmt.Sprintf("æœåŠ¡å™¨è¿›ç¨‹ %s å·²å¯åŠ¨ï¼", processName),
				Action:    "è¿›ç¨‹å·²å¯åŠ¨ï¼Œè¯·è§‚å¯Ÿæ˜¯å¦æœ‰å¼‚å¸¸ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "recovery", data)
			(*statusMap)[processName] = 0
			log.Printf("æœåŠ¡å™¨ %s è¿›ç¨‹ %s å·²å¯åŠ¨ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€", address, processName)
			//fmt.Println(data.Message)
		}
		statusMutex.Unlock()
	}
}

// å¤„ç†èµ„æºçŠ¶æ€
func handleResourceStatus(address, key string, metrics Metrics, server Server, status *ServerStatus, config Config) {
	// åŸå­æ“ä½œç®¡ç†çŠ¶æ€
	cpuAlertSent := atomic.LoadInt32(&status.CpuAlertSent) == 1
	// ä½¿ç”¨äº’æ–¥ä¿æŠ¤èµ„æºç›¸å…³çŠ¶æ€
	status.ResourceMutex.Lock()
	defer status.ResourceMutex.Unlock()

	// é…ç½®å‚æ•°ï¼ˆå¯æ”¹ä¸ºé…ç½®é¡¹ï¼‰
	consecutiveToAlert := config.ConsecutiveToAlert     // è¿ç»­è¶…è¿‡é˜ˆå€¼æ¬¡æ•°æ‰å‘Šè­¦
	consecutiveToRecover := config.ConsecutiveToRecover // è¿ç»­æ­£å¸¸æ¬¡æ•°æ‰æ¢å¤
	emaAlpha := config.ResourceSmooth                   // EMA å¹³æ»‘ç³»æ•°ï¼Œ0<alpha<=1ï¼Œalpha è¶Šå¤§å“åº”è¶Šå¿«ï¼Œè¶Šå°è¶Šå¹³æ»‘

	// è·å–å½“å‰ CPU å€¼ï¼ˆåŸå§‹ï¼‰
	currentCPU := metrics.CPUUsage

	// å¦‚æœ LastCPUEMA == 0 åˆ™è®¤ä¸ºæœªåˆå§‹åŒ–ï¼Œç›´æ¥è®¾ä¸ºå½“å‰å€¼
	if status.LastCPUEMA == 0 {
		status.LastCPUEMA = currentCPU
	} else {
		// æŒ‡æ•°å¹³æ»‘ï¼ˆEMAï¼‰
		status.LastCPUEMA = emaAlpha*currentCPU + (1-emaAlpha)*status.LastCPUEMA
	}

	smoothedCPU := status.LastCPUEMA

	// ä½¿ç”¨å¹³æ»‘å€¼è¿›è¡Œåˆ¤æ–­ï¼ˆé¿å…å•æ¬¡ spikeï¼‰
	cpuThreshold := server.CPUThreshold

	// è¶…é˜ˆå€¼å¤„ç†ï¼ˆå»æŠ–ï¼‰
	if smoothedCPU > cpuThreshold {
		status.CpuHighCount++
		status.CpuNormalCount = 0
		if status.CpuHighCount >= consecutiveToAlert && !cpuAlertSent {
			data := EmailTemplateData{
				Subject:   "âš ï¸CPUä½¿ç”¨ç‡å‘Šè­¦",
				Server:    address,
				Message:   "CPUä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼ˆå¹³æ»‘å€¼ï¼‰",
				Value:     fmt.Sprintf("%.2f%% (smoothed)", smoothedCPU),
				Threshold: fmt.Sprintf("%.2f%%", cpuThreshold),
				Action:    "è¯·æ£€æŸ¥æœåŠ¡å™¨è´Ÿè½½æƒ…å†µï¼Œå¿…è¦æ—¶è¿›è¡Œæ‰©å®¹æˆ–ä¼˜åŒ–ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "warning", data)
			log.Printf("æœåŠ¡å™¨ %s CPUåˆ©ç”¨ç‡(å¹³æ»‘)=%.2f è¶…è¿‡é˜ˆå€¼ %.2fï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€", address, smoothedCPU, cpuThreshold)
			atomic.StoreInt32(&status.CpuAlertSent, 1)
			// é‡ç½®è®¡æ•°é¿å…é‡å¤å‘é€
			status.CpuHighCount = 0
		}
	} else {
		// æ­£å¸¸å€¼è®¡æ•°ï¼ˆç”¨äºæ¢å¤ï¼‰
		status.CpuNormalCount++
		status.CpuHighCount = 0
		if status.CpuNormalCount >= consecutiveToRecover && cpuAlertSent {
			data := EmailTemplateData{
				Subject:   "âœ…CPUä½¿ç”¨ç‡å·²é™ä½",
				Server:    address,
				Message:   "CPUä½¿ç”¨ç‡å·²æ¢å¤æ­£å¸¸ï¼ˆå¹³æ»‘å€¼ï¼‰",
				Value:     fmt.Sprintf("%.2f%% (smoothed)", smoothedCPU),
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "recovery", data)
			log.Printf("æœåŠ¡å™¨ %s CPUåˆ©ç”¨ç‡(å¹³æ»‘)=%.2f æ¢å¤ä½äºé˜ˆå€¼ %.2fï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€", address, smoothedCPU, cpuThreshold)
			atomic.StoreInt32(&status.CpuAlertSent, 0)
			status.CpuNormalCount = 0
		}
	}

	memAlertSent := atomic.LoadInt32(&status.MemAlertSent) == 1

	if metrics.MemoryUsage > server.MemoryThreshold && !memAlertSent {
		data := EmailTemplateData{
			Subject:   "âš ï¸å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦",
			Server:    address,
			Message:   "å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼",
			Value:     fmt.Sprintf("%.2f%%", metrics.MemoryUsage),
			Threshold: fmt.Sprintf("%.2f%%", server.MemoryThreshold),
			Action:    "è¯·æ£€æŸ¥æœåŠ¡å™¨è¿›ç¨‹å ç”¨å†…å­˜æƒ…å†µï¼Œå¿…è¦æ—¶è¿›ç¨‹å†…å­˜åˆ†é…ä¼˜åŒ–æˆ–æ‰©å®¹ï¼",
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "warning", data)
		log.Printf("æœåŠ¡å™¨ %s å†…å­˜åˆ©ç”¨ç‡å·²è¾¾åˆ° %s è¶…è¿‡é˜ˆå€¼ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€",
			address, metrics.MemoryUsage)
		atomic.StoreInt32(&status.MemAlertSent, 1)
	} else if metrics.MemoryUsage < server.MemoryThreshold && memAlertSent {
		data := EmailTemplateData{
			Subject:   "âœ…å†…å­˜ä½¿ç”¨ç‡å·²é™ä½",
			Server:    address,
			Message:   "æœåŠ¡å™¨å†…å­˜ä½¿ç”¨ç‡å·²é™ä½ï¼Œæ¢å¤æ­£å¸¸ï¼",
			Value:     fmt.Sprintf("%.2f%%", metrics.MemoryUsage),
			Timestamp: time.Now().Format("2006-01-02 15:04:05"),
		}
		sendEmail(config.Email, "recovery", data)
		log.Printf("æœåŠ¡å™¨ %s å†…å­˜åˆ©ç”¨ç‡å·²é™åˆ° %s ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€",
			address, metrics.MemoryUsage)
		atomic.StoreInt32(&status.MemAlertSent, 0)
	}

	// ç£ç›˜çŠ¶æ€å¤„ç†
	diskMutex := &status.DiskMutex
	diskMap := &status.DiskAlertSent

	diskMutex.Lock()

	for mountpoint, usage := range metrics.DiskUsage {
		if isExcludedMountPoint(mountpoint, server.ExcludeMountPoints) {
			continue
		}

		alertSent, exists := (*diskMap)[mountpoint]
		if !exists {
			(*diskMap)[mountpoint] = 0
			alertSent = 0
		}

		if usage > server.DiskThreshold && alertSent == 0 {
			data := EmailTemplateData{
				Subject:   "âš ï¸ç£ç›˜ä½¿ç”¨ç‡å‘Šè­¦",
				Server:    address,
				Message:   "ç£ç›˜åˆ©ç”¨ç‡è¶…è¿‡é˜ˆå€¼ï¼",
				Value:     fmt.Sprintf("%.2f%% æŒ‚è½½ç‚¹ï¼š%s", usage, mountpoint),
				Threshold: fmt.Sprintf("%.2f%%", server.DiskThreshold),
				Action:    "è¯·æ£€æŸ¥æœåŠ¡å™¨ç£ç›˜åˆ†åŒºä½¿ç”¨æƒ…å†µï¼Œå¿…è¦æ—¶è¿›è¡Œæ¸…ç†æˆ–æ‰©å®¹ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "warning", data)
			log.Printf("æœåŠ¡å™¨ %s æŒ‚è½½ç‚¹ %s åˆ©ç”¨ç‡å·²è¾¾åˆ° %s è¶…è¿‡é˜ˆå€¼ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€",
				address, mountpoint, usage)
			(*diskMap)[mountpoint] = 1
		} else if usage < server.DiskThreshold && alertSent == 1 {
			data := EmailTemplateData{
				Subject:   "âœ…ç£ç›˜åˆ©ç”¨ç‡å·²é™ä½",
				Server:    address,
				Message:   "æœåŠ¡å™¨ç£ç›˜åˆ©ç”¨ç‡å·²é™ä½ï¼Œæ¢å¤æ­£å¸¸ï¼",
				Value:     fmt.Sprintf("%.2f%% æŒ‚è½½ç‚¹ï¼š%s", usage, mountpoint),
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "recovery", data)
			log.Printf("æœåŠ¡å™¨ %s æŒ‚è½½ç‚¹ %s åˆ©ç”¨ç‡å·²é™åˆ° %s ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€",
				address, mountpoint, usage)
			(*diskMap)[mountpoint] = 0
		}
	}
	diskMutex.Unlock()
}

// å¤„ç†ç›®å½•çŠ¶æ€
func handleDirectoryStatus(address, key string, dirs []DirectoryStatus, status *ServerStatus, config Config, now time.Time) {
	dirMutex := &status.DirMutex
	dirMap := &status.DirAlertSent
	fileMap := &status.FileAlertSent

	dirMutex.Lock()
	defer dirMutex.Unlock()

	for _, result := range dirs {
		baseDir := result.BaseDir
		folderExists := result.DirectoryExist
		fileActuallyExists := result.XdrFileExist // âœ… ä½¿ç”¨æ¥å£è¿”å›çš„æ–‡ä»¶å­˜åœ¨çŠ¶æ€

		// åˆå§‹åŒ–ç›®å½•å‘Šè­¦çŠ¶æ€
		dirAlertSent, ok1 := (*dirMap)[baseDir]
		if !ok1 {
			(*dirMap)[baseDir] = 0
			dirAlertSent = 0
		}

		// åˆå§‹åŒ–æ–‡ä»¶å‘Šè­¦çŠ¶æ€
		fileAlertSent, ok2 := (*fileMap)[baseDir]
		if !ok2 {
			(*fileMap)[baseDir] = 0
			fileAlertSent = 0
		}

		// 1. ç›®å½•/æ–‡ä»¶ç¼ºå¤±å‘Šè­¦
		if !folderExists && !fileActuallyExists && dirAlertSent == 0 {
			data := EmailTemplateData{
				Subject:   "âš ï¸âš ï¸ç›®å½•/æ–‡ä»¶ç¼ºå¤±å‘Šè­¦",
				Server:    address,
				Message:   fmt.Sprintf("æœåŠ¡å™¨ç›®å½• %s ä¸å­˜åœ¨æŒ‡å®šç›®å½•æˆ–æ–‡ä»¶ï¼", baseDir),
				Action:    "è¯·æ£€æŸ¥æœåŠ¡å™¨è¾“å‡ºæ–‡ä»¶è¿›ç¨‹æ˜¯å¦æ­£å¸¸è¿è¡Œï¼",
				Timestamp: now.Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "warning", data)
			log.Printf("æœåŠ¡å™¨ %s ç›®å½• %s ä¸å­˜åœ¨æŒ‡å®šç›®å½•/æ–‡ä»¶ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€",
				address, baseDir)
			(*dirMap)[baseDir] = 1
		} else if (folderExists || fileActuallyExists) && dirAlertSent == 1 {
			data := EmailTemplateData{
				Subject:   "âœ…ç›®å½•/æ–‡ä»¶å·²æ¢å¤",
				Server:    address,
				Message:   fmt.Sprintf("æœåŠ¡å™¨ç›®å½• %s æ–‡ä»¶/ç›®å½•å·²æ¢å¤ï¼", baseDir),
				Timestamp: now.Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "recovery", data)
			log.Printf("æœåŠ¡å™¨ %s ç›®å½• %s ç›®å½•/æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€",
				address, baseDir)
			(*dirMap)[baseDir] = 0
		}

		// 2. æ–‡ä»¶ç¼ºå¤±å‘Šè­¦ï¼ˆç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ç¼ºå¤±ï¼‰
		if folderExists {
			if !fileActuallyExists && fileAlertSent == 0 {
				data := EmailTemplateData{
					Subject:   "âš ï¸âš ï¸æ–‡ä»¶ç¼ºå¤±å‘Šè­¦",
					Server:    address,
					Message:   fmt.Sprintf("æœåŠ¡å™¨ç›®å½• %s ä¸å­˜åœ¨æŒ‡å®šæ–‡ä»¶ï¼", baseDir),
					Action:    "è¯·æ£€æŸ¥æœåŠ¡å™¨è¾“å‡ºæˆ–è½¬ç§»æ–‡ä»¶è¿›ç¨‹æ˜¯å¦æ­£å¸¸è¿è¡Œï¼",
					Timestamp: now.Format("2006-01-02 15:04:05"),
				}
				sendEmail(config.Email, "warning", data)
				log.Printf("æœåŠ¡å™¨ %s ç›®å½• %s ä¸å­˜åœ¨æŒ‡å®šæ–‡ä»¶ï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€",
					address, baseDir)
				(*fileMap)[baseDir] = 1
			} else if fileActuallyExists && fileAlertSent == 1 {
				data := EmailTemplateData{
					Subject:   "âœ…ç›®å½•æ–‡ä»¶å·²æ¢å¤",
					Server:    address,
					Message:   fmt.Sprintf("æœåŠ¡å™¨ç›®å½• %s å†…æ–‡ä»¶å·²æ¢å¤ï¼", baseDir),
					Timestamp: now.Format("2006-01-02 15:04:05"),
				}
				sendEmail(config.Email, "recovery", data)
				log.Printf("æœåŠ¡å™¨ %s ç›®å½• %s æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€",
					address, baseDir)
				(*fileMap)[baseDir] = 0
			}
		}
	}
}

// å¤„ç†ç›®æ ‡ç«¯å£çŠ¶æ€
func handleTargetPortStatus(address, key string, ports []PortStatus, status *ServerStatus, config Config) {
	portMutex := &status.TargetPortMutex
	portMap := &status.TargetPortAlertSent

	portMutex.Lock()
	defer portMutex.Unlock()

	for _, result := range ports {
		hostName := result.Host
		targetPort := result.Port
		portState := result.Status

		alertSent, exists := (*portMap)[hostName]
		if !exists {
			(*portMap)[hostName] = 0
			alertSent = 0
		}

		if !portState && alertSent == 0 {
			data := EmailTemplateData{
				Subject:   "âš ï¸ç›®æ ‡æœåŠ¡å™¨ä¸ä¸‹æ¸¸ç«¯å£å¤±è”å‘Šè­¦",
				Server:    address,
				Message:   fmt.Sprintf("ä»æœåŠ¡å™¨ %s åˆ°ç›®æ ‡æœåŠ¡å™¨ %s çš„ %d ç«¯å£é€šä¿¡å¤±è”ï¼Œè¯·æ ¸æŸ¥ï¼", address, hostName, targetPort),
				Action:    "è¯·ç™»å½•æœåŠ¡å™¨æ£€æŸ¥ç«¯å£é€šä¿¡æ˜¯å¦æ­£å¸¸ï¼Œå¦åˆ™å½±å“ç›¸å…³é€šä¿¡ä¼ è¾“ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "severe", data)
			log.Printf("æœåŠ¡å™¨ %s è®¿é—® %s çš„ç«¯å£ %s ä¸é€šï¼Œè¯·ç¡®è®¤å‘Šè­¦é‚®ä»¶å·²å‘é€",
				address, hostName, targetPort)
			(*portMap)[hostName] = 1
		} else if portState && alertSent == 1 {
			data := EmailTemplateData{
				Subject:   "âœ…ç›®æ ‡æœåŠ¡å™¨ä¸ä¸‹æ¸¸ç«¯å£é€šä¿¡æ¢å¤",
				Server:    address,
				Message:   fmt.Sprintf("ä»æœåŠ¡å™¨ %s åˆ°ç›®æ ‡æœåŠ¡å™¨ %s çš„ %d ç«¯å£é€šä¿¡å·²æ¢å¤æ­£å¸¸ã€‚", address, hostName, targetPort),
				Action:    "ä¸ç›®æ ‡æœåŠ¡å™¨çš„ç«¯å£é€šä¿¡å·²æ¢å¤æ­£å¸¸ï¼Œè¯·çŸ¥æ‚‰ï¼",
				Timestamp: time.Now().Format("2006-01-02 15:04:05"),
			}
			sendEmail(config.Email, "recovery", data)
			log.Printf("ä»æœåŠ¡å™¨ %s åˆ° %s çš„ç«¯å£ %s é€šä¿¡æ¢å¤ï¼Œè¯·ç¡®è®¤æ¢å¤é‚®ä»¶å·²å‘é€",
				address, hostName, targetPort)
			(*portMap)[hostName] = 0
		}
	}
}
